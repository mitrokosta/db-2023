# Анализ на предмет CAP

## DragonFly

В основной [документации](https://github.com/dragonflydb/dragonfly/blob/main/README.md) репликация заявлена, как фича в роадмапе и не задокументирована.
Однако, в релизе 0.14.0 промелькнул такой релизноут:
> Replication is almost ready - one can set up a master/slave setup with a usual "replicaof" command on a secondary replica (FAILOVER is not supported yet)

Для проверки сценария использован docker-compose. Используются 3 контейнера: два инстанса Dragonfly (df1 и df2) и один инстанс redis клиента (redis), для подключения к базе.
1. Поднимем контейнеры `docker-compose -f dragonfly.yml up -d`.
2. Подключимся к инстансу df1 `docker exec -it redis redis-cli -h df1`. Эту сессию назовем сессией 1.
3. Подключимся к инстансу df2 `docker exec -it redis redis-cli -h df2`. Эту сессию назовем сессией 2.
4. В сессии 2 сделаем df2 репликой df1 `replicaof df1 6379`. Получим `OK`.
5. В сессии 1 создадим пару ключ/значение `set hello world`.
6. В сессии 2 получим значение по ключу `get hello`. Получим `"world"`. Репликация работает.
7. Отсоединим df2 от сети df1-df2 `docker network disconnect df1-df2 df2`.
8. В сессии 1 изменим значение ключа hello `set hello notworld`.
9. В сессии 2 получим значение ключа hello `get hello`. Получим старое значение `world`.
10. Вернем df2 в сеть df1-df2 `docker network connect df1-df2 df2`.
11. Подождем минуту для синхронизации реплики.
11. В сессии 2 получим значение ключа hello `get hello`. Получим новое значение `notworld`.

Отмечу, что в любых условиях DragonFly отказывается писать данные в реплику: `(error) READONLY You can't write against a read only replica.`.

Итак, в условиях разрыва соединения система продолжает работать в штатном режиме, но данные из реплик могут быть не самыми свежими. Таким образом, эта база данных относится к **AP**.

## ScyllaDB

Для проверки сценария использован docker-compose. Используются 4 контейнера: три инстанса Scylla (sc1, sc2, sc3) и один инстанс Cassandra (client), для подключения к Scylla.

Отмечу, что Scylla довольно капризна в условиях поднятия нескольких инстансов на одной машине, поэтому конфигурации в docker-compose побольше. Кроме того, пришлось увеличить значение `fs.aio-max-nr` в `/etc/sysctl.conf`. 
1. Поднимем контейнеры `docker-compose -f scylla.yml up -d`.
2. Подключимся к инстансу sc1 `docker exec -it client cqlsh sc1`. Эту сессию назовем сессией 1.
3. Подключимся к инстансу sc2 `docker exec -it client cqlsh sc2`. Эту сессию назовем сессией 2.
4. Подключимся к инстансу sc3 `docker exec -it client cqlsh sc3`. Эту сессию назовем сессией 3.
5. В сессии 1 создадим тестовый кейспейс `create keyspace ks with replication = {'class': 'SimpleStrategy', 'replication_factor': 3};`.
6. Во всех сессиях переключимся на новый кейспейс `use ks;`.
7. В сессии 2 создадим тестовую таблицу `create table t (k text primary key);`.
8. В сессии 3 добавим в таблицу новую строчку `insert into t (k) values ('1');`.
9. Проверим, что во всех сессиях в таблице появилась строчка `select k from t;`. Во всех сессиях получим одну строчку `1`.
10. Отсоединим sc3 от сети servers `docker network disconnect servers sc3`.
11. В сессии 1 добавим в таблицу новую строчку `insert into t (k) values ('2');`.
12. Проверим, что в сессиях 1 и 2 строчка появилась, а в сессии 3 нет `select k from t;`. В сессиях 1 и 2 получим строчки `1` и `2`, а в сессии 3 только `1`.
13. В сессии 3 добавим в таблицу новую строчку `insert into t (k) values ('3');`.
14. Проверим, что в сессии 3 строчка появилась, а в сессиях 1 и 2 нет `select k from t;`. В сессиях 1 и 2 получим строчки `1` и `2`, а в сессии 3 `1` и `3`.
15. Вернем sc3 в сеть servers `docker network connect servers sc3`.
16. Подождем минуту для синхронизации.
17. Проверим, что во всех сессиях данные объединились и одинаковы `select k from t;`. Во всех сессиях получим строчки `1`, `2` и `3`.

Итак, в условиях разрыва соединения система функционирует в штатном режиме, в каждую ноду можно писать, из каждой ноды можно читать, но данные на нодах могут отличаться. Таким образом, эта база данных относится к **AP**.

## ArenadataDB

Согласно [документации Arenadata DB](https://docs.arenadata.io/adb/index.html), кластер состоит из мастер-ноды, резервных мастер-нод, сегментов с данными и зеркальных сегментов. Доступ к данным есть только через главную мастер-ноду, сегменты не принимают клиентские соединения напрямую. Резервные мастер-ноды не активны, пока главная жива. Зеркальные сегменты не используются до тех пор, пока обычные сегменты живы.
Поскольку данные не хранятся на мастер-ноде, в случае отказа сегмента мастер не сможет вернуть клиенту никакие данные, пока сегмент не оживет или его место не займет резервный сегмент. Следовательно, эта база данных относится к **CP**.
